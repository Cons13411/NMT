{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.8.0 in /opt/conda/lib/python3.7/site-packages (0.8.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.8.0) (4.50.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.8.0) (1.18.5)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext==0.8.0) (1.7.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.8.0) (2.24.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchtext==0.8.0) (3.7.4.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.0) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.0) (3.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import codecs\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "!pip install torchtext==0.8.0 \n",
    "from torchtext.data.functional import generate_sp_model,load_sp_model, sentencepiece_numericalizer, sentencepiece_tokenizer\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import torchtext\n",
    "import copy\n",
    "import sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 22 22:06:18 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   59C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The field class is going to depricated soon, so there are constant warnings about it, this is to shut them down.\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our tokenizer\n",
    "def tokenizer(txt):\n",
    "    txt = txt.lower()\n",
    "    tokenized = word_tokenize(txt)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definning the field with the custom tokenizer\n",
    "src_sentence_field = data.Field(sequential=True, tokenize=tokenizer, batch_first=True)\n",
    "trg_sentence_field = data.Field(sequential=True, tokenize=tokenizer,\n",
    "                           init_token='<s>', eos_token='</s>', batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading pair sentence using TabularDataset\n",
    "dataset_translation = data.TabularDataset(path='~/../cons13411/trainset.txt', format='TSV',\n",
    "                                         fields=[('src',src_sentence_field), ('trg', trg_sentence_field)],\n",
    "                                         )\n",
    "trainset, valset, testset = dataset_translation.split(split_ratio=[0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,i in enumerate(dataset_translation):\n",
    "    try:\n",
    "        if i.trg:\n",
    "            continue\n",
    "        else:\n",
    "            print (i.src)\n",
    "    except:\n",
    "        print(i.src)\n",
    "#         dataset_translation.examples[j].trg = ['...', '...']\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the BucketBatch iterator \n",
    "src_sentence_field.build_vocab(dataset_translation)\n",
    "trg_sentence_field.build_vocab(dataset_translation)\n",
    "train_iterator, val_iterator, test_iterator = data.BucketIterator.splits((trainset,valset,testset), (128,64,32),\n",
    "                                                                         sort_key= lambda x:len(x.src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layer, num_hidden_size, embed_size, vocab_size, gpu , dropout = 0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_size = num_hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.gpu = gpu\n",
    "        self.dropout_v = dropout\n",
    "        self.dropout = nn.Dropout(self.dropout_v)\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size,self.embed_size)\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, self.hidden_size, self.num_layer,\n",
    "                            batch_first=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        embeddings = self.embedding(input)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        output, hidden = self.GRU(embeddings)\n",
    "        #print(f\"encoder output size: {output.shape}\")\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.gpu:\n",
    "              hidden_state = torch.zeros(self.num_layer,batch_size,self.hidden_size).cuda()\n",
    "         # cell_state = torch.randn(self.num_layer,batch_size,self.hidden_size).cuda()\n",
    "        else: \n",
    "              hidden_state = torch.zeros(self.num_layer,batch_size,self.hidden_size)\n",
    "          #cell_state = torch.randn(self.num_layer,batch_size,self.hidden_size)\n",
    "        #hidden = (hidden_state,cell_state)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder Class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layer, num_hidden_size, embed_size, vocab_size, gpu, dropout=0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_size = num_hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.gpu = gpu\n",
    "        self.embedding = nn.Embedding(self.vocab_size,self.embed_size)\n",
    "        self.GRU = nn.GRU(self.hidden_size, self.hidden_size, self.num_layer,\n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        if len(input.size()) == 1:\n",
    "            input = input.unsqueeze(1)\n",
    "\n",
    "        embeddings = self.embedding(input)\n",
    "        embeddings = F.relu(embeddings)\n",
    "        output, hidden = self.GRU(embeddings, hidden)\n",
    "        decoder_output = self.linear(output)\n",
    "        return decoder_output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(src_seq, encoder, trg_seq, decoder, criterion, trg_vocab, gpu=False, teacher_forcing_ratio = 0.5):\n",
    "\n",
    "        encoder_output, encoder_hidden = encoder(src_seq)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        batch_size = trg_seq.shape[0]\n",
    "        target_out_label = trg_seq[:,:]\n",
    "        target_in_label = trg_seq[:,1:-1]\n",
    "        decoder_input = torch.tensor(np.full((batch_size,1), trg_vocab.stoi['<s>']))\n",
    "        if gpu:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        batch_loss = 0\n",
    "        batch_words = 0\n",
    "        batch_correct = 0\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            for tg_id in range(len(target_in_label)):\n",
    "                    if tg_id == target_in_label.shape[1]:\n",
    "                          break\n",
    "                  # print(decoder_input.get_device())\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                    tg_word = target_out_label[:,tg_id+1]\n",
    "                    batch_loss += criterion(decoder_output.squeeze(), tg_word)\n",
    "                    decoder_input = target_in_label[:,tg_id]\n",
    "          # if gpu:\n",
    "          #   decoder_input.cuda()\n",
    "        else:\n",
    "\n",
    "            for tg_id in range(len(target_in_label)):\n",
    "                if tg_id == target_in_label.shape[1]:\n",
    "                        break\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                tg_word = target_out_label[:,tg_id+1]\n",
    "                batch_loss += criterion(decoder_output.squeeze(), tg_word)\n",
    "                predict = F.log_softmax(decoder_output.detach(),dim=2).topk(1)[1]\n",
    "                decoder_input = predict.squeeze().detach()\n",
    "          # if gpu:\n",
    "          #   decoder_input.cuda()\n",
    "          # if predict == trg_vocab.stoi['</s>']:\n",
    "          #   break\n",
    "      \n",
    "\n",
    "        return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate( encoder, decoder, eval_iterator, criterion, trg_vocab, epoch, gpu=False, max_len=40):\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        total_words = 0\n",
    "        total_correct = 0\n",
    "        accu = np.inf\n",
    "\n",
    "        with torch.no_grad():\n",
    "            losses = []\n",
    "            total = 0\n",
    "            for batch in eval_iterator:\n",
    "                encoder.zero_grad()\n",
    "                decoder.zero_grad()\n",
    "                batch_loss = 0\n",
    "                batch_words = 0\n",
    "                batch_correct = 0\n",
    "                \n",
    "                src_batch = batch.src\n",
    "                trg_batch = batch.trg\n",
    "              \n",
    "                if gpu:\n",
    "                    src_batch = src_batch.cuda()\n",
    "                    trg_batch = trg_batch.cuda()\n",
    "                \n",
    "                if len(src_batch.size()) == 1:\n",
    "                    continue\n",
    "\n",
    "                target_out_label = trg_batch[:,1:]\n",
    "                batch_size = trg_batch.size(0)\n",
    "\n",
    "                encoder_output, encoder_hidden = encoder(src_batch)\n",
    "                decoder_hidden = encoder_hidden\n",
    "                batch_size = trg_batch.shape[0]\n",
    "                target_out_label = trg_batch[:,1:]\n",
    "                decoder_input = torch.tensor(np.full((batch_size,1), trg_vocab.stoi['<s>']))\n",
    "                if gpu:\n",
    "                    decoder_input = decoder_input.cuda()\n",
    "                predictions = []\n",
    "                batch_loss = 0\n",
    "                batch_words = 0\n",
    "                batch_correct = 0\n",
    "                for tg_id in range(max_len):\n",
    "                    if tg_id == target_out_label.shape[1]:\n",
    "                          break\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                    tg_word = target_out_label[:,tg_id]\n",
    "                    batch_loss += criterion(decoder_output.squeeze(), tg_word)\n",
    "                    predict = F.log_softmax(decoder_output.detach(),dim=2).topk(1)[1]\n",
    "                    decoder_input = predict.squeeze().detach()\n",
    "                    predictions.append(decoder_input)\n",
    "                    if epoch % 5 == 0:\n",
    "                            batch_words += torch.sum(torch.tensor([1 if tg_word[i] != trg_vocab.stoi['<PAD>']\n",
    "                                                                else 0 for i in range(len(tg_word))]))\n",
    "                            batch_correct += torch.sum(torch.tensor([1 if predict[i] == tg_word[i] and \n",
    "                                                                    tg_word[i] != trg_vocab.stoi['<PAD>'] \n",
    "                                                                    else 0 for i in range(len(tg_word)) ]))\n",
    "                    \n",
    "                avg_batch_loss = batch_loss.item()/max_len\n",
    "                losses.append(avg_batch_loss)\n",
    "                if epoch % 5 == 0:\n",
    "                    total_words += batch_words\n",
    "                    total_correct += batch_correct\n",
    "                total += 1\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            accu = float(total_correct)/total_words\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        return (sum(losses) / total), accu, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter settings\n",
    "num_layer = 1\n",
    "num_hidden_size = 64\n",
    "embed_size = 64\n",
    "encode_vocab_size = len(src_sentence_field.vocab)\n",
    "decoder_vocab_size = len(trg_sentence_field.vocab)\n",
    "#encoder_dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep    0 |Train loss 5.264678 |Evaluation loss 12.608212 |Eval Acc 0.246238|Saved\n",
      "\n",
      "Ep    1 |Train loss 4.470560 |Evaluation loss 16.054450 |Eval Acc  inf\n",
      "\n",
      "Ep    2 |Train loss 4.091588 |Evaluation loss 13.936067 |Eval Acc  inf\n",
      "\n",
      "Ep    3 |Train loss 3.566999 |Evaluation loss 7.777138 |Eval Acc  inf|Saved\n",
      "\n",
      "Ep    4 |Train loss 3.749710 |Evaluation loss 18.233942 |Eval Acc  inf\n",
      "\n",
      "Ep    5 |Train loss 3.694788 |Evaluation loss 5.535477 |Eval Acc 0.254998|Saved\n",
      "\n",
      "Ep    6 |Train loss 3.694913 |Evaluation loss 7.896628 |Eval Acc  inf\n",
      "\n",
      "Ep    7 |Train loss 3.806899 |Evaluation loss 5.990084 |Eval Acc  inf\n",
      "\n",
      "Ep    8 |Train loss 3.094708 |Evaluation loss 10.173778 |Eval Acc  inf\n",
      "\n",
      "Ep    9 |Train loss 3.453951 |Evaluation loss 5.366072 |Eval Acc  inf|Saved\n",
      "\n",
      "Ep   10 |Train loss 3.484871 |Evaluation loss 6.674027 |Eval Acc 0.254750\n",
      "\n",
      "Ep   11 |Train loss 3.168108 |Evaluation loss 11.224906 |Eval Acc  inf\n",
      "\n",
      "Ep   12 |Train loss 3.488645 |Evaluation loss 6.712248 |Eval Acc  inf\n",
      "\n",
      "Ep   13 |Train loss 3.170822 |Evaluation loss 7.524651 |Eval Acc  inf\n",
      "\n",
      "Ep   14 |Train loss 3.251143 |Evaluation loss 6.553896 |Eval Acc  inf\n",
      "\n",
      "Ep   15 |Train loss 3.158209 |Evaluation loss 8.866743 |Eval Acc 0.255050\n",
      "\n",
      "Ep   16 |Train loss 3.148754 |Evaluation loss 5.032902 |Eval Acc  inf|Saved\n",
      "\n",
      "Ep   17 |Train loss 3.161742 |Evaluation loss 9.249841 |Eval Acc  inf\n",
      "\n",
      "Ep   18 |Train loss 3.004953 |Evaluation loss 10.914548 |Eval Acc  inf\n",
      "\n",
      "Ep   19 |Train loss 3.311703 |Evaluation loss 7.577921 |Eval Acc  inf\n",
      "\n",
      "Ep   20 |Train loss 3.278100 |Evaluation loss 8.096083 |Eval Acc 0.256932\n",
      "\n",
      "Ep   21 |Train loss 3.588889 |Evaluation loss 6.565725 |Eval Acc  inf\n",
      "\n",
      "Ep   22 |Train loss 3.431073 |Evaluation loss 7.301336 |Eval Acc  inf\n",
      "\n",
      "Ep   23 |Train loss 3.298007 |Evaluation loss 8.171291 |Eval Acc  inf\n",
      "\n",
      "Ep   24 |Train loss 3.548777 |Evaluation loss 8.716630 |Eval Acc  inf\n",
      "\n",
      "Ep   25 |Train loss 3.570522 |Evaluation loss 9.328760 |Eval Acc 0.253222\n",
      "\n",
      "Ep   26 |Train loss 3.532106 |Evaluation loss 9.045904 |Eval Acc  inf\n",
      "\n",
      "Ep   27 |Train loss 3.647142 |Evaluation loss 5.156428 |Eval Acc  inf\n",
      "\n",
      "Ep   28 |Train loss 3.524273 |Evaluation loss 5.359404 |Eval Acc  inf\n",
      "\n",
      "Ep   29 |Train loss 2.900714 |Evaluation loss 8.394275 |Eval Acc  inf\n",
      "\n",
      "Ep   30 |Train loss 2.992476 |Evaluation loss 5.102787 |Eval Acc 0.256199\n",
      "\n",
      "Ep   31 |Train loss 3.561820 |Evaluation loss 6.075501 |Eval Acc  inf\n",
      "\n",
      "Ep   32 |Train loss 2.773862 |Evaluation loss 7.841441 |Eval Acc  inf\n",
      "\n",
      "Ep   33 |Train loss 2.993256 |Evaluation loss 5.015107 |Eval Acc  inf|Saved\n",
      "\n",
      "Ep   34 |Train loss 2.867039 |Evaluation loss 5.712361 |Eval Acc  inf\n",
      "\n",
      "Ep   35 |Train loss 3.188571 |Evaluation loss 5.928651 |Eval Acc 0.255686\n",
      "\n",
      "Ep   36 |Train loss 2.709047 |Evaluation loss 11.719134 |Eval Acc  inf\n",
      "\n",
      "Ep   37 |Train loss 2.662381 |Evaluation loss 4.908574 |Eval Acc  inf|Saved\n",
      "\n",
      "Ep   38 |Train loss 2.622120 |Evaluation loss 5.243429 |Eval Acc  inf\n",
      "\n",
      "Ep   39 |Train loss 2.631910 |Evaluation loss 5.189686 |Eval Acc  inf\n",
      "\n",
      "Ep   40 |Train loss 2.689092 |Evaluation loss 7.029156 |Eval Acc 0.253683\n",
      "\n",
      "Ep   41 |Train loss 2.669456 |Evaluation loss 8.024450 |Eval Acc  inf\n",
      "\n",
      "Ep   42 |Train loss 2.831490 |Evaluation loss 8.944499 |Eval Acc  inf\n",
      "\n",
      "Ep   43 |Train loss 2.656246 |Evaluation loss 6.390749 |Eval Acc  inf\n",
      "\n",
      "Ep   44 |Train loss 2.639453 |Evaluation loss 5.815431 |Eval Acc  inf\n",
      "\n",
      "Ep   45 |Train loss 2.662527 |Evaluation loss 6.519026 |Eval Acc 0.253186\n",
      "\n",
      "Ep   46 |Train loss 2.728093 |Evaluation loss 5.640128 |Eval Acc  inf\n",
      "\n",
      "Ep   47 |Train loss 2.858118 |Evaluation loss 5.299991 |Eval Acc  inf\n",
      "\n",
      "Ep   48 |Train loss 2.755586 |Evaluation loss 7.292724 |Eval Acc  inf\n",
      "\n",
      "Ep   49 |Train loss 2.853974 |Evaluation loss 5.849966 |Eval Acc  inf\n",
      "\n",
      "Ep   50 |Train loss 2.777235 |Evaluation loss 5.684867 |Eval Acc 0.128534\n",
      "\n",
      "Ep   51 |Train loss 2.618844 |Evaluation loss 5.857174 |Eval Acc  inf\n",
      "\n",
      "Ep   52 |Train loss 2.680828 |Evaluation loss 5.878998 |Eval Acc  inf\n",
      "\n",
      "Ep   53 |Train loss 2.655923 |Evaluation loss 5.737140 |Eval Acc  inf\n",
      "\n",
      "Ep   54 |Train loss 2.637281 |Evaluation loss 5.625563 |Eval Acc  inf\n",
      "\n",
      "Ep   55 |Train loss 2.631160 |Evaluation loss 5.305574 |Eval Acc 0.200073\n",
      "\n",
      "Ep   56 |Train loss 2.607888 |Evaluation loss 6.189264 |Eval Acc  inf\n",
      "\n",
      "Ep   57 |Train loss 2.498763 |Evaluation loss 5.954612 |Eval Acc  inf\n",
      "\n",
      "Ep   58"
     ]
    }
   ],
   "source": [
    "#Train Procedure \n",
    "gpu_device = False\n",
    "if torch.cuda.is_available():\n",
    "    torch.device('cuda')\n",
    "    gpu_device = True\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "encoder = Encoder(num_layer,num_hidden_size,embed_size,encode_vocab_size, gpu_device)\n",
    "decoder = Decoder(num_layer,num_hidden_size,embed_size,decoder_vocab_size, gpu_device)\n",
    "\n",
    "if gpu_device:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_sentence_field.vocab['<PAD>'])\n",
    "# optimencoder = torch.optim.Adam(encoder.parameters(),lr=0.001, weight_decay=0.0, betas=(0.9, 0.999),\n",
    "#                          eps=1e-8, amsgrad=False) \n",
    "# optimdecoder = torch.optim.Adam(decoder.parameters(),lr=0.001, weight_decay=0.0, betas=(0.9, 0.999),\n",
    "#                          eps=1e-8, amsgrad=False) \n",
    "optimencoder = torch.optim.SGD(encoder.parameters(), lr= 0.01)\n",
    "optimdecoder = torch.optim.SGD(decoder.parameters(), lr= 0.01)\n",
    "\n",
    "epoch = 100\n",
    "\n",
    "least_loss = np.inf\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    print('Ep {:4d}'.format(i), end='')\n",
    "    losses = []\n",
    "    total = 0\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for batch in train_iterator:\n",
    "\n",
    "        src_batch = batch.src\n",
    "        trg_batch = batch.trg\n",
    "        \n",
    "        if gpu_device:\n",
    "            src_batch = src_batch.cuda()\n",
    "            trg_batch = trg_batch.cuda()\n",
    "\n",
    "\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "\n",
    "        encoder.train(mode = True)\n",
    "        decoder.train(mode = True)\n",
    "        \n",
    "        if len(src_batch.size()) == 1:\n",
    "            continue        \n",
    "\n",
    "        batch_loss = train(src_batch, encoder, trg_batch, decoder, criterion,\n",
    "                           trg_sentence_field.vocab, gpu=gpu_device, teacher_forcing_ratio=0.5)\n",
    "            \n",
    "        batch_loss.backward()\n",
    "        optimencoder.step()\n",
    "        optimdecoder.step()\n",
    "        avg_loss = batch_loss.item()/trg_batch.size(1)\n",
    "        losses.append(avg_loss)\n",
    "        # total_correct += batch_correct\n",
    "        # total_words += batch_word\n",
    "        #print(f\"accuracy batch: {float(batch_correct)/batch_word}\")\n",
    "        total += 1\n",
    "      \n",
    "    epoch_loss = np.sum(losses)/total\n",
    "    train_loss.append(epoch_loss)\n",
    "    # accuracy = float(total_correct)/total_words\n",
    "    print(' |Train loss {:4f}'.format(epoch_loss), end='')\n",
    "    eval_loss, eval_accu, predictions = evaluate(encoder, decoder, val_iterator,\n",
    "                                                       criterion, trg_sentence_field.vocab,\n",
    "                                                       i, gpu=gpu_device)\n",
    "    print(' |Evaluation loss {:4f}'.format(eval_loss), end='')\n",
    "    print(' |Eval Acc {:4f}'.format(eval_accu), end='')\n",
    "    if least_loss > eval_loss :\n",
    "        least_loss = eval_loss\n",
    "        torch.save(encoder.state_dict(), './encoder_noattention_sgd.pth')\n",
    "        torch.save(decoder.state_dict(), './decoder_noattention_sgd.pth')\n",
    "        best_encoder = copy.deepcopy(encoder)\n",
    "        best_decoder = copy.deepcopy(decoder)\n",
    "        print('|Saved\\n')\n",
    "    else:\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfering a tensor to a sentence (BPE encoding)\n",
    "def denumericalization(vocab, tensor):\n",
    "    sentence = ''\n",
    "    for index in tensor:\n",
    "        if index in [1,2,3]:\n",
    "            continue\n",
    "\n",
    "        sentence += ' '+ vocab.itos[index]\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(encoder,decoder, input_sequence, src_sentence_field, trg_sentence_field, temperature = 0.0 ):\n",
    "    \n",
    "    inpute_tokenized = src_sentence_field.tokenize(input_sequence)\n",
    "    input_seq = src_sentence_field.numericalize([inpute_tokenized])\n",
    "    \n",
    "    target_tokens = [trg_sentence_field.vocab.stoi['<s>']]\n",
    "    \n",
    "    target_seq = torch.tensor(np.full((1,1), trg_sentence_field.vocab.stoi['<s>'] ))\n",
    "    EOS = trg_sentence_field.vocab.stoi['</s>']\n",
    "    \n",
    "    trg_tokens = prediction(encoder,decoder,input_seq, target_seq, EOS, temperature=temperature)\n",
    "    \n",
    "    print(trg_tokens)\n",
    "\n",
    "    return trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def prediction(encoder, decoder, input_seq, target_seq, EOS, temperature=0.0):\n",
    "        \n",
    "        target_tokens = [] \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            encoder.zero_grad()\n",
    "            decoder.zero_grad()\n",
    "\n",
    "\n",
    "            batch_size = target_seq.size(0)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                input_seq = input_seq.cuda()\n",
    "                target_seq = target_seq.cuda()\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_seq)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_output, decoder_hidden = decoder(target_seq, decoder_hidden)\n",
    "            \n",
    "            symbol = logsoftmax_sample(decoder_output, temperature=temperature)\n",
    "            symbol_list = symbol.tolist()[0][0]\n",
    "            target_tokens.append(symbol_list[0])\n",
    "            counter = 0 \n",
    "            while symbol != EOS and counter < 100 :\n",
    "                # print(f\"{symbol}  : {trg_sentence_field.vocab.itos[symbol]}\")\n",
    "                # print(decoder_output)\n",
    "                target_seq = torch.tensor(np.full((1,1), symbol_list[0] ))\n",
    "                if torch.cuda.is_available():\n",
    "                    target_seq = target_seq.cuda()\n",
    "                decoder_output, decoder_hidden = decoder(target_seq, decoder_hidden)\n",
    "                symbol = logsoftmax_sample(decoder_output, temperature=temperature)\n",
    "                symbol_list = symbol.tolist()[0][0]\n",
    "                target_tokens.append(symbol_list[0])\n",
    "                counter +=1\n",
    "\n",
    "                \n",
    "            return target_tokens\n",
    "        \n",
    "\n",
    "    def logsoftmax_sample(logits, temperature=1.0):  \n",
    "\n",
    "        u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=logits.shape)\n",
    "        g = -np.log(-np.log(u))\n",
    "        g = torch.from_numpy(g)\n",
    "        if torch.cuda.is_available():\n",
    "            g = g.cuda()\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        probs =log_probs + g * temperature\n",
    "\n",
    "        return probs.topk(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_device = False\n",
    "if torch.cuda.is_available():\n",
    "    torch.device('cuda')\n",
    "    gpu_device = True\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "encoder = Encoder(num_layer,num_hidden_size,embed_size,encode_vocab_size, gpu_device)\n",
    "decoder = Decoder(num_layer,num_hidden_size,embed_size,decoder_vocab_size, gpu_device)\n",
    "\n",
    "if gpu_device:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_sentence_field.vocab['<PAD>'])\n",
    "\n",
    "encoder.load_state_dict(torch.load('./encoder_noattention.pth'))\n",
    "decoder.load_state_dict(torch.load('./decoder_noattention.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 14, 19, 4, 101, 18, 4, 6, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'wenn wir nicht , sondern daß , .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = sampling(encoder, decoder, 'We do not know what is happening.', src_sentence_field, trg_sentence_field, temperature=0.3)\n",
    "denumericalization(trg_sentence_field.vocab, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = ['wenn', 'wir', 'nicht',  ',', 'sondern', 'daß', ',' ,'.']\n",
    "ref = [['wir', 'wissen', 'nicht', ',', 'was', 'passiert', '.']]\n",
    "sentence_bleu(ref, trans, weights=(0,1,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
